{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forests (Breiman, 2001) \n",
    "\n",
    "L'algorithme random forest ou \"Random Forest\" est ce que l'on appelle un meta-estimateur. Car celui-ci utilise un ensemble de d'arbre de décision pour effectuer une prédiction. Cette prédiction résulte de la moyenne des prédiction individuelle de chaque arbre. Pour être efficace \"Random Forest\" combine deux méthodes d'échantillonage le \"bagging\" et la selection de sous-espace aléatoire \"random features selection\". L'objectif étant de minimiser le surapprentissage (overfitting) en maximisant la précision \"accuracy\".\n",
    "\n",
    "<img src=\"./tree.jpg\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deux concepts clefs le bagging et la selection de sous-espace aléatoire\n",
    "\n",
    "##### Problématique\n",
    "L'un des problème qu'il est néccessaire de corriger pour que le principe d'ensemble fonctionne est celui de la corrélation entre les arbres. Si tous les arbres sont identiques, la foret d'arbre est alors équivalent à un unique arbre de décision. Il est donc néccessaire d'obtenir des arbres différents (qui ont une faible corrélation) . Ce problème est accentuer lorsque l'on et dans un contexte Big-Data, ayant accès a des Datasets très très large avec une grande dimentionnalité. \n",
    "Breiman dans son papier introduit la notion de force et corrélation, affirmant que l'objectif est de maximiser la \"force\" ou l'éfficacité de chaque arbre tout en minimisant la corrélation entre ceux-ci. Pour ce faire 2 stratégies sont alors mise en place :\n",
    "\n",
    "## 1. Bagging\n",
    "\n",
    "Considérant un jeux de donnée d'entrainement $ D $ de taille n, le processus de bagging génère $ m $ nouveaux jeux de données $ D_i $, chacun de taille $ n' $, en échantillonnant $ D $ de manière uniforme et avec remplacement. \n",
    "\n",
    "En échantillonant avec remplacement, certaines observations peuvent être répétés de nombreuses fois au sein de chaque $D_i$ .\n",
    "Par convention, si $ n$ est grand on prend $ n = n'$ et dès lors le jeux de donnée $D_i$ contient $ (1 - \\frac{1}{e}) $ $ \\approx 63.2% $\n",
    "d'échantillon unique, le reste étant des duplicatas. Puis dans le cas du Random Forest, $m$ arbres de décisions sont entrainés en utilisant les $ m $ jeux de données ci-dessus et combinés en moyennant la sortie (pour la régressions) ou en effectuant un vote majoritaire (pour la classification).\n",
    "\n",
    "## 2. Selection de sous-espace aléatoire \"random features selection\"\n",
    "\n",
    "Le principe est le suivant, pour chaque arbre composant la \"foret\" et pour chaque noeud de ces arbres on choisit un nombre $ j $ de dimentions aléatoire (features).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
